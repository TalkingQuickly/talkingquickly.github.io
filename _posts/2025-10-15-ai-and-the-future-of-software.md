---
layout: post
title: "AI and the Future of Software: Thoughts on LLMs and What's Next"
date: 2025-10-15
permalink: /ai-and-the-future-of-software/
biofooter: true
---

There's lots of talk about a bubble vs not but unless you happen to be fundraising right now, it's probably one of the less important questions in startups and AI.

This is in addition to [unreasonable things I believe about large language models](/unreasonable-things-i-believe-about-llms/).

My rough beliefs as of September 2025 are:

- LLM's represent a fundamental shift in technology (e.g. long term this isn't all hype)
- A medium term impact is that the market for software gets much bigger because software can now do things currently being done by people (see [Sequoia video](https://www.sequoiacap.com/article/10t-ai-revolution/) or [A16Z podcast](https://open.spotify.com/episode/4QCjZlEzijkVWEQY8L4sIL?si=07e7059da5674c1a))
- Long term impacts are probably much bigger and incredibly hard to reason about
- Everything only really got started in March of this year (2025) with models like Opus, everything before that was practice, so we haven't really seen "real" AI products in the wild yet outside of developer tooling
- The "super fast to $100m ARR with 1 person" thing is a distraction, over time they will get more competition and so have to invest in sales, marketing and more fundamental product areas so burn multiples and fundamentals generally will return to the levels we expect
- Of course there's a bubble, it's a blip in the long term but still a large amount of capital will be lost where there have been high valuations for fairly simple AI wrappers. It's not that they're bad products, it's just that there's a big economic difference between "it's fast if you're the first to do something useful and there's nobody else in the market" and "there's a persistent first mover advantage once there are competitors in the market"
- The next wave is how to actually apply this in the Enterprise to generate a real ROI, we've barely seen any of this yet. This is taking fundamental systems of records and core workflow systems and imagining them completely
-  Chat and voice will be an unreasonably large point of this, the death of chat as a UI is not just over-stated, it's wrong
- One of the next big frontiers will be the availability of training data, e.g. in robotics there's a lack of structured training data for "doing basic tasks" and even in Enterprise workflows there's a lack of structured training data on "what does doing x workflow well look like". So finding ways to create or access this will be as or more impactful as improvements in foundation models. Companies that have proprietary data of this kind will have an unfair advantage
- Over the next 12 months attention will shift from huge foundation models to fine tunes of smaller models
- There's a huge amount of innovation coming down the pipe in terms of model size and inference efficiency
- Foundation models are in a precarious position given the trajectory of open models. There's a risk of a "Docker" type situation here where un-intuitively the technology turns out to be too foundational for it to be provided by a company. For this to be true there would have to be a meaningful shift in the economics of training models or a plateau in performance that leads to all focus being on fine tuning

For me that all adds up to "we've barely scratched the surface of what LLM's can do and how much better they can make the world".